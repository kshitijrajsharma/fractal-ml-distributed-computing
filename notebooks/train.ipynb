{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3e8a42d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from sparkmeasure import StageMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "70f58ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = \"/opt/spark/work-dir/data/FRACTAL\"\n",
    "data_path = \"s3a://ubs-datasets/FRACTAL/data\"\n",
    "sample_fraction = 0.000001\n",
    "num_executors = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ed623c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"fractal-rf\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.DefaultAWSCredentialsProviderChain\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.instances\", str(num_executors)) \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .config(\"spark.eventLog.enabled\", \"true\") \\\n",
    "    .config(\"spark.eventLog.dir\", \"/opt/spark/spark-events\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e8e302fb-73ae-4964-a323-772cc2b2944d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark UI: http://2b8282c8bdeb:4040\n"
     ]
    }
   ],
   "source": [
    "stage_metrics = StageMetrics(spark)\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0abd5e1d-597e-477f-bcfb-c0b925d791e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df):\n",
    "    return df.withColumn(\"z_raw\", col(\"xyz\")[2]) \\\n",
    "        .withColumn(\n",
    "            \"ndvi\",\n",
    "            when(\n",
    "                (col(\"Infrared\") + col(\"Red\")) != 0,\n",
    "                (col(\"Infrared\") - col(\"Red\")) / (col(\"Infrared\") + col(\"Red\")),\n",
    "            ).otherwise(0),\n",
    "        ) \\\n",
    "        .select(\n",
    "            \"z_raw\", \"Intensity\", \"Red\", \"Green\", \"Blue\", \"Infrared\", \"ndvi\",\n",
    "            col(\"Classification\").alias(\"label\"),\n",
    "        )\n",
    "\n",
    "\n",
    "def load_sample(spark, path, fraction, cols):\n",
    "    print(f\"Loading data from {path} with fraction={fraction}\")\n",
    "    \n",
    "    sc = spark.sparkContext\n",
    "    hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "    \n",
    "    uri = sc._jvm.java.net.URI(path)\n",
    "    fs = sc._jvm.org.apache.hadoop.fs.FileSystem.get(uri, hadoop_conf)\n",
    "    file_path = sc._jvm.org.apache.hadoop.fs.Path(path)\n",
    "    \n",
    "    all_files = [\n",
    "        str(f.getPath()) for f in fs.listStatus(file_path)\n",
    "        if str(f.getPath()).endswith(\".parquet\")\n",
    "    ]\n",
    "    \n",
    "    num_files = max(1, int(len(all_files) * fraction))\n",
    "    selected_files = sorted(all_files)[:num_files]\n",
    "    \n",
    "    print(f\"Loading {num_files}/{len(all_files)} files ({fraction*100:.1f}%)\")\n",
    "    \n",
    "    df = spark.read.parquet(*selected_files).select(*cols)\n",
    "    df = prepare_data(df)\n",
    "    row_count = df.count()\n",
    "    \n",
    "    if row_count == 0:\n",
    "        raise ValueError(f\"No data loaded from {path}. Check data path and fraction.\")\n",
    "    \n",
    "    print(f\"Loaded {row_count} rows\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e90e7e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets\n",
      "Loading data from s3a://ubs-datasets/FRACTAL/data/train/ with fraction=1e-06\n",
      "Loading 1/80000 files (0.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 31408 rows\n",
      "Loading data from s3a://ubs-datasets/FRACTAL/data/val/ with fraction=1e-06\n",
      "Loading 1/10000 files (0.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 80164 rows\n",
      "Loading data from s3a://ubs-datasets/FRACTAL/data/test/ with fraction=1e-06\n",
      "Loading 1/10000 files (0.0%)\n",
      "Loaded 107172 rows\n"
     ]
    }
   ],
   "source": [
    "cols = [\"xyz\", \"Intensity\", \"Classification\", \"Red\", \"Green\", \"Blue\", \"Infrared\"]\n",
    "\n",
    "stage_metrics.begin()\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Loading datasets\")\n",
    "train = load_sample(spark, f\"{data_path}/train/\", sample_fraction, cols)\n",
    "val = load_sample(spark, f\"{data_path}/val/\", sample_fraction, cols)\n",
    "test = load_sample(spark, f\"{data_path}/test/\", sample_fraction, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a28e18c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_assembler = VectorAssembler(\n",
    "    inputCols=[\"z_raw\"], outputCol=\"z_vec\", handleInvalid=\"skip\"\n",
    ")\n",
    "z_scaler = StandardScaler(\n",
    "    inputCol=\"z_vec\", outputCol=\"z\", withMean=False, withStd=True\n",
    ")\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"z\", \"Intensity\", \"Red\", \"Green\", \"Blue\", \"Infrared\", \"ndvi\"],\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\",\n",
    ")\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    numTrees=5,\n",
    "    maxDepth=20,\n",
    "    seed=62\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[z_assembler, z_scaler, assembler, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "795b70fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/10 21:55:53 WARN DAGScheduler: Broadcasting large task binary with size 1005.8 KiB\n",
      "25/11/10 21:55:53 WARN DAGScheduler: Broadcasting large task binary with size 1108.6 KiB\n",
      "25/11/10 21:55:53 WARN DAGScheduler: Broadcasting large task binary with size 1189.5 KiB\n",
      "25/11/10 21:55:53 WARN DAGScheduler: Broadcasting large task binary with size 1254.7 KiB\n",
      "25/11/10 21:55:54 WARN DAGScheduler: Broadcasting large task binary with size 1303.5 KiB\n",
      "25/11/10 21:55:54 WARN DAGScheduler: Broadcasting large task binary with size 1341.1 KiB\n",
      "25/11/10 21:55:54 WARN DAGScheduler: Broadcasting large task binary with size 1366.7 KiB\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf0b7d3",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "53f2277d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 64:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val: 0.0002, Test: 0.2720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "val_predictions = model.transform(val)\n",
    "val_accuracy = evaluator.evaluate(val_predictions)\n",
    "\n",
    "test_predictions = model.transform(test)\n",
    "test_accuracy = evaluator.evaluate(test_predictions)\n",
    "\n",
    "print(f\"Val: {val_accuracy:.4f}, Test: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9e751b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 67.02s\n"
     ]
    }
   ],
   "source": [
    "stage_metrics.end()\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total time: {total_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "079aae36-74fb-4ad5-9d11-e0dcb0b93f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'numStages': 61, 'numTasks': 64, 'elapsedTime': 36573, 'stageDuration': 26662, 'executorRunTime': 25800, 'executorCpuTime': 4875, 'executorDeserializeTime': 1235, 'executorDeserializeCpuTime': 1082, 'resultSerializationTime': 13, 'jvmGCTime': 80, 'shuffleFetchWaitTime': 14, 'shuffleWriteTime': 7, 'resultSize': 127813, 'diskBytesSpilled': 0, 'memoryBytesSpilled': 0, 'peakExecutionMemory': 43704929, 'recordsRead': 1163968, 'bytesRead': 100428630, 'recordsWritten': 0, 'bytesWritten': 0, 'shuffleRecordsRead': 4089, 'shuffleTotalBlocksFetched': 30, 'shuffleLocalBlocksFetched': 29, 'shuffleRemoteBlocksFetched': 1, 'shuffleTotalBytesRead': 1364636, 'shuffleLocalBytesRead': 1364577, 'shuffleRemoteBytesRead': 59, 'shuffleRemoteBytesReadToDisk': 0, 'shuffleBytesWritten': 1364636, 'shuffleRecordsWritten': 4089}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage_metrics.aggregate_stagemetrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bf749281",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.unpersist()\n",
    "val.unpersist()\n",
    "test.unpersist()\n",
    "val_predictions.unpersist()\n",
    "test_predictions.unpersist()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a247749-400f-4612-bb40-0334fb18598f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
