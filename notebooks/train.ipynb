{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e8a42d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from sparkmeasure import StageMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70f58ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/opt/spark/work-dir/data/FRACTAL\"\n",
    "sample_fraction = 0.000001\n",
    "num_executors = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed623c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/10 20:39:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"fractal-rf\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.DefaultAWSCredentialsProviderChain\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.instances\", str(num_executors)) \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .config(\"spark.eventLog.enabled\", \"true\") \\\n",
    "    .config(\"spark.eventLog.dir\", \"/opt/spark/spark-events\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8e302fb-73ae-4964-a323-772cc2b2944d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark UI: http://2b8282c8bdeb:4040\n"
     ]
    }
   ],
   "source": [
    "stage_metrics = StageMetrics(spark)\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d33d2fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df):\n",
    "    df = df.withColumn(\"z_raw\", col(\"xyz\")[2]).withColumn(\n",
    "        \"ndvi\",\n",
    "        when(\n",
    "            (col(\"Infrared\") + col(\"Red\")) != 0,\n",
    "            (col(\"Infrared\") - col(\"Red\")) / (col(\"Infrared\") + col(\"Red\")),\n",
    "        ).otherwise(0),\n",
    "    )\n",
    "    return df.select(\n",
    "        \"z_raw\",\n",
    "        \"Intensity\",\n",
    "        \"Red\",\n",
    "        \"Green\",\n",
    "        \"Blue\",\n",
    "        \"Infrared\",\n",
    "        \"ndvi\",\n",
    "        col(\"Classification\").alias(\"label\"),\n",
    "    )\n",
    "\n",
    "\n",
    "def load_sample(spark, path, fraction, cols):\n",
    "    df = spark.read.parquet(path).select(*cols).sample(fraction=fraction, seed=62)\n",
    "    df = prepare_data(df).cache()\n",
    "    print(f\"Loaded {df.count()} rows from {path}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e90e7e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7350 rows from /opt/spark/work-dir/data/FRACTAL/train/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 912 rows from /opt/spark/work-dir/data/FRACTAL/val/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:=====================================================>(594 + 2) / 596]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 969 rows from /opt/spark/work-dir/data/FRACTAL/test/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "cols = [\"xyz\", \"Intensity\", \"Classification\", \"Red\", \"Green\", \"Blue\", \"Infrared\"]\n",
    "\n",
    "stage_metrics.begin()\n",
    "start_time = time.time()\n",
    "\n",
    "train = load_sample(spark, f\"{data_path}/train/\", sample_fraction, cols)\n",
    "val = load_sample(spark, f\"{data_path}/val/\", sample_fraction, cols)\n",
    "test = load_sample(spark, f\"{data_path}/test/\", sample_fraction, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a28e18c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_assembler = VectorAssembler(\n",
    "    inputCols=[\"z_raw\"], outputCol=\"z_vec\", handleInvalid=\"skip\"\n",
    ")\n",
    "z_scaler = StandardScaler(\n",
    "    inputCol=\"z_vec\", outputCol=\"z\", withMean=False, withStd=True\n",
    ")\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"z\", \"Intensity\", \"Red\", \"Green\", \"Blue\", \"Infrared\", \"ndvi\"],\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\",\n",
    ")\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    numTrees=5,\n",
    "    maxDepth=20,\n",
    "    seed=62\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[z_assembler, z_scaler, assembler, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "795b70fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/10 20:44:28 WARN DAGScheduler: Broadcasting large task binary with size 1113.2 KiB\n",
      "25/11/10 20:45:26 WARN DAGScheduler: Broadcasting large task binary with size 1807.0 KiB\n",
      "25/11/10 20:46:47 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "25/11/10 20:48:28 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "25/11/10 20:50:27 WARN DAGScheduler: Broadcasting large task binary with size 5.4 MiB\n",
      "25/11/10 20:52:36 WARN DAGScheduler: Broadcasting large task binary with size 7.0 MiB\n",
      "25/11/10 20:54:39 WARN DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "25/11/10 20:56:28 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "25/11/10 20:58:15 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "25/11/10 20:59:51 WARN DAGScheduler: Broadcasting large task binary with size 11.9 MiB\n",
      "25/11/10 21:01:13 WARN DAGScheduler: Broadcasting large task binary with size 12.7 MiB\n",
      "25/11/10 21:02:26 WARN DAGScheduler: Broadcasting large task binary with size 13.4 MiB\n",
      "25/11/10 21:03:28 WARN DAGScheduler: Broadcasting large task binary with size 13.9 MiB\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf0b7d3",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53f2277d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/10 21:04:21 WARN DAGScheduler: Broadcasting large task binary with size 9.3 MiB\n",
      "25/11/10 21:04:23 WARN DAGScheduler: Broadcasting large task binary with size 9.3 MiB\n",
      "[Stage 67:================================================>     (531 + 4) / 596]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val: 0.6601, Test: 0.6667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "val_predictions = model.transform(val)\n",
    "val_accuracy = evaluator.evaluate(val_predictions)\n",
    "\n",
    "test_predictions = model.transform(test)\n",
    "test_accuracy = evaluator.evaluate(test_predictions)\n",
    "\n",
    "print(f\"Val: {val_accuracy:.4f}, Test: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e751b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 1514.09s\n"
     ]
    }
   ],
   "source": [
    "stage_metrics.end()\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total time: {total_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf749281",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.unpersist()\n",
    "val.unpersist()\n",
    "test.unpersist()\n",
    "val_predictions.unpersist()\n",
    "test_predictions.unpersist()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c7aa52-15ca-43d7-a9c1-70749caaf04e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
