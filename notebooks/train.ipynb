{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f094383a-e24d-4677-ad4e-a3f67a314fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fda7605d-eb3c-4142-a451-126e281a23c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/05 07:22:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"fractal-cv-rf\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.DefaultAWSCredentialsProviderChain\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4a960ae-7e7b-4df2-8138-3048fd075ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2118cbb-da1c-452d-9c81-6761d2144ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "parq_cols = [\"xyz\", \"Intensity\", \"Classification\", \"Red\", \"Green\", \"Blue\", \"Infrared\"]\n",
    "data_path = \"/opt/spark/work-dir/data/FRACTAL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cce9a33-a0c4-4a70-b3f5-7e537be69c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df):\n",
    "    df = df.withColumn(\"z\", col(\"xyz\")[2])\n",
    "    df = df.withColumn(\"ndvi\", when(\n",
    "        (col(\"Infrared\") + col(\"Red\")) != 0,\n",
    "        (col(\"Infrared\") - col(\"Red\")) / (col(\"Infrared\") + col(\"Red\"))\n",
    "    ).otherwise(0))\n",
    "    \n",
    "    feature_assembler = VectorAssembler(\n",
    "        inputCols=[\"z\", \"Intensity\", \"Red\", \"Green\", \"Blue\", \"Infrared\", \"ndvi\"],\n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "    \n",
    "    df = feature_assembler.transform(df)\n",
    "    return df.select(\"features\", col(\"Classification\").alias(\"label\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9a22ff5-1454-404b-be08-17029dd74853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sample(path, fraction=0.2): # get 20 percent of the data\n",
    "    return prepare_data(spark.read.parquet(path).select(*parq_cols).sample(fraction=fraction, seed=42))\n",
    "\n",
    "train = load_sample(f\"{data_path}/train/\")\n",
    "val = load_sample(f\"{data_path}/val/\")\n",
    "test = load_sample(f\"{data_path}/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e4dd09f-d310-446e-874c-6c852422819a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/05 07:30:25 WARN DAGScheduler: Broadcasting large task binary with size 1453.6 KiB\n",
      "25/11/05 07:32:11 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/11/05 07:34:21 WARN DAGScheduler: Broadcasting large task binary with size 5.7 MiB\n",
      "25/11/05 07:37:01 WARN DAGScheduler: Broadcasting large task binary with size 1916.6 KiB\n",
      "25/11/05 07:37:15 WARN DAGScheduler: Broadcasting large task binary with size 9.3 MiB\n",
      "25/11/05 07:39:57 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/11/05 07:40:15 WARN DAGScheduler: Broadcasting large task binary with size 9.5 MiB\n",
      "25/11/05 07:42:06 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/11/05 07:42:24 WARN DAGScheduler: Broadcasting large task binary with size 10.0 MiB\n",
      "25/11/05 07:43:50 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/11/05 07:44:07 WARN DAGScheduler: Broadcasting large task binary with size 10.0 MiB\n",
      "25/11/05 07:45:33 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/11/05 07:45:52 WARN DAGScheduler: Broadcasting large task binary with size 10.1 MiB\n",
      "25/11/05 07:47:42 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/11/05 07:48:01 WARN DAGScheduler: Broadcasting large task binary with size 10.5 MiB\n",
      "25/11/05 07:49:27 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/11/05 07:49:46 WARN DAGScheduler: Broadcasting large task binary with size 11.0 MiB\n",
      "25/11/05 07:51:16 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/11/05 07:51:35 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n",
      "25/11/05 07:52:38 WARN DAGScheduler: Broadcasting large task binary with size 6.4 MiB\n",
      "25/11/05 07:53:44 WARN DAGScheduler: Broadcasting large task binary with size 1680.3 KiB\n",
      "25/11/05 07:53:56 WARN DAGScheduler: Broadcasting large task binary with size 7.8 MiB\n",
      "25/11/05 07:55:01 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "25/11/05 07:55:15 WARN DAGScheduler: Broadcasting large task binary with size 30.2 MiB\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best: {'numTrees': 50, 'maxDepth': 10, 'maxBins': 32} -> Accuracy: 0.7419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/05 08:20:45 WARN DAGScheduler: Broadcasting large task binary with size 1455.3 KiB\n",
      "25/11/05 08:22:51 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/11/05 08:25:31 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "25/11/05 08:27:59 WARN DAGScheduler: Broadcasting large task binary with size 1563.4 KiB\n",
      "25/11/05 08:28:17 WARN DAGScheduler: Broadcasting large task binary with size 4.8 MiB\n",
      "25/11/05 08:30:01 WARN DAGScheduler: Broadcasting large task binary with size 1563.4 KiB\n",
      "25/11/05 08:30:19 WARN DAGScheduler: Broadcasting large task binary with size 5.0 MiB\n",
      "25/11/05 08:31:36 WARN DAGScheduler: Broadcasting large task binary with size 1563.4 KiB\n",
      "25/11/05 08:31:52 WARN DAGScheduler: Broadcasting large task binary with size 5.5 MiB\n",
      "25/11/05 08:32:56 WARN DAGScheduler: Broadcasting large task binary with size 1563.4 KiB\n",
      "25/11/05 08:33:12 WARN DAGScheduler: Broadcasting large task binary with size 5.3 MiB\n",
      "25/11/05 08:34:15 WARN DAGScheduler: Broadcasting large task binary with size 1563.4 KiB\n",
      "25/11/05 08:34:32 WARN DAGScheduler: Broadcasting large task binary with size 5.3 MiB\n",
      "25/11/05 08:35:49 WARN DAGScheduler: Broadcasting large task binary with size 1563.4 KiB\n",
      "25/11/05 08:36:06 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "25/11/05 08:37:10 WARN DAGScheduler: Broadcasting large task binary with size 1563.4 KiB\n",
      "25/11/05 08:37:27 WARN DAGScheduler: Broadcasting large task binary with size 5.7 MiB\n",
      "25/11/05 08:38:34 WARN DAGScheduler: Broadcasting large task binary with size 1563.4 KiB\n",
      "25/11/05 08:38:51 WARN DAGScheduler: Broadcasting large task binary with size 5.3 MiB\n",
      "25/11/05 08:40:29 WARN DAGScheduler: Broadcasting large task binary with size 1563.4 KiB\n",
      "25/11/05 08:40:46 WARN DAGScheduler: Broadcasting large task binary with size 5.5 MiB\n",
      "25/11/05 08:42:03 WARN DAGScheduler: Broadcasting large task binary with size 1563.4 KiB\n",
      "25/11/05 08:42:21 WARN DAGScheduler: Broadcasting large task binary with size 5.7 MiB\n",
      "25/11/05 08:43:26 WARN DAGScheduler: Broadcasting large task binary with size 1563.4 KiB\n",
      "25/11/05 08:43:43 WARN DAGScheduler: Broadcasting large task binary with size 5.7 MiB\n",
      "25/11/05 08:44:52 WARN DAGScheduler: Broadcasting large task binary with size 1563.4 KiB\n",
      "25/11/05 08:45:08 WARN DAGScheduler: Broadcasting large task binary with size 6.3 MiB\n",
      "25/11/05 08:46:17 WARN DAGScheduler: Broadcasting large task binary with size 1563.4 KiB\n",
      "25/11/05 08:46:34 WARN DAGScheduler: Broadcasting large task binary with size 5.5 MiB\n",
      "25/11/05 08:47:50 WARN DAGScheduler: Broadcasting large task binary with size 1563.4 KiB\n",
      "25/11/05 08:48:07 WARN DAGScheduler: Broadcasting large task binary with size 5.2 MiB\n",
      "25/11/05 08:49:24 WARN DAGScheduler: Broadcasting large task binary with size 1563.4 KiB\n",
      "25/11/05 08:49:41 WARN DAGScheduler: Broadcasting large task binary with size 4.6 MiB\n",
      "25/11/05 08:50:49 WARN DAGScheduler: Broadcasting large task binary with size 1204.2 KiB\n",
      "25/11/05 08:51:03 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "25/11/05 08:52:15 WARN DAGScheduler: Broadcasting large task binary with size 1563.4 KiB\n",
      "25/11/05 08:52:33 WARN DAGScheduler: Broadcasting large task binary with size 5.6 MiB\n",
      "25/11/05 08:53:39 WARN DAGScheduler: Broadcasting large task binary with size 1522.8 KiB\n",
      "25/11/05 08:53:55 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/11/05 08:54:40 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/11/05 08:55:20 WARN DAGScheduler: Broadcasting large task binary with size 31.9 MiB\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best: {'numTrees': 50, 'maxDepth': 10, 'maxBins': 64} -> Accuracy: 0.7427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/05 09:22:30 WARN DAGScheduler: Broadcasting large task binary with size 1453.6 KiB\n",
      "25/11/05 09:24:27 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/11/05 09:26:50 WARN DAGScheduler: Broadcasting large task binary with size 5.7 MiB\n",
      "ERROR:root:KeyboardInterrupt while sending command.            (100 + 20) / 574]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n",
      "[Stage 126:=========>                                          (101 + 20) / 574]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 19\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m max_bins \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m64\u001b[39m]:\n\u001b[1;32m     10\u001b[0m     rf \u001b[38;5;241m=\u001b[39m RandomForestClassifier(\n\u001b[1;32m     11\u001b[0m         labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     12\u001b[0m         featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m         seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     17\u001b[0m     )\n\u001b[0;32m---> 19\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mrf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39mevaluate(model\u001b[38;5;241m.\u001b[39mtransform(val))\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accuracy \u001b[38;5;241m>\u001b[39m best_accuracy:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "best_accuracy = 0\n",
    "best_params = {}\n",
    "\n",
    "rf_template = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", seed=42)\n",
    "\n",
    "for num_trees in [50, 100, 200]:\n",
    "    for max_depth in [10, 15, 20]:\n",
    "        for max_bins in [32, 64]:\n",
    "            rf = RandomForestClassifier(\n",
    "                labelCol=\"label\", \n",
    "                featuresCol=\"features\", \n",
    "                numTrees=num_trees,\n",
    "                maxDepth=max_depth,\n",
    "                maxBins=max_bins,\n",
    "                seed=42\n",
    "            )\n",
    "            \n",
    "            model = rf.fit(val)\n",
    "            accuracy = evaluator.evaluate(model.transform(val))\n",
    "            \n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_params = {\"numTrees\": num_trees, \"maxDepth\": max_depth, \"maxBins\": max_bins}\n",
    "                print(f\"New best: {best_params} -> Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7cda2f-4b4a-4d85-a691-8ec1c9147524",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nBest Params: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe32a010-17a0-4b00-b2f3-a1443050645b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = RandomForestClassifier(\n",
    "    labelCol=\"label\", \n",
    "    featuresCol=\"features\",\n",
    "    numTrees=best_params[\"numTrees\"],\n",
    "    maxDepth=best_params[\"maxDepth\"],\n",
    "    maxBins=best_params[\"maxBins\"],\n",
    "    seed=42\n",
    ").fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2ac74d-ce71-439b-a7c1-1a94b61bb521",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/05 09:27:40 ERROR Instrumentation: org.apache.spark.SparkException: Job 69 cancelled because SparkContext was shut down\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1259)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1257)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1257)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:3129)\n",
      "\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$3(DAGScheduler.scala:3015)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:3015)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2258)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2258)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2211)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$34(SparkContext.scala:681)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:995)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:738)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:737)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:663)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:208)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:302)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:168)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:139)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:47)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test Accuracy: {evaluator.evaluate(best_model.transform(test)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e05ec70-51a7-444c-b00b-fe264b9a2c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9618e245-699f-4cd3-b674-849172876f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
